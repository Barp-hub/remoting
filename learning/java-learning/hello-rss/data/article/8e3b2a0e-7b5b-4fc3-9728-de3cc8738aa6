<p>当你看到这篇文章的时候，想必你已经从某个渠道已经了解到了Spark，这里 贴一下<a href="http://spark.apache.org/docs/latest/">官方的介绍</a>吧：</p>
<blockquote><p>Apache Spark is a <strong>fast</strong> and <strong>general-purpose cluster computing</strong> system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, <a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLlib</a> for machine learning, <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.<br />
<span id="more-1915"></span></p></blockquote>
<p>简单来说，它跟hadoop是同一类东西，作为大数据计算的框架，只是hadoop必须依赖hdfs存储，而Spark可依赖内存去迭代计算，当然快啊，其实spark也可以读取或者存储数据到hdfs。 这个玩意诞生于加州大学伯克利分校的，2010年开源出来。 我写这篇文章的时候最新版本已经到了1.6.2， 发展算比较快吧。</p>
<p>上面这段英文里面加粗的是Spark的周边产品，例如<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>就是用来做流式计算，类似storm,只不过这个更粗粒度，有兴趣可以深究，但是今天仅仅介绍一下Spark的基本安装和操作。</p>
<p>每个人使用Spark的目的不一样， 我研究Spark主要是为了研究一下R语言在这上面运行的情况。从1.4开始，spark加入了R语言的支持，名叫sparkR。</p>
<p><a href="https://www.r-project.org/">R语言</a>你可以理解成一个免费的软件，软件里面已经帮你实现了很多算法，例如回归算法，求指数，求对数算法。这款软件在学术界用的比较多，因为你可以直接在里面写公式求结果，非常简单。同时，你也可以通过Java程序去调用R语言的API。</p>
<p>&#160;</p>
<p><strong>安装准备：</strong></p>
<p>1.  OS： linux</p>
<p>2. 下载安装包<a href="http://www.apache.org/dyn/closer.lua/spark/spark-1.6.2/spark-1.6.2.tgz">spark-1.6.2.tgz</a>：http://spark.apache.org/downloads.html  直接linux机器上解压就好了。</p>
<p>3. 准备2台linux机器apc-lgdcdevl201，apc-lgdcdevl202，进程分布如下：</p>
<blockquote><p>apc-lgdcdevl201: master , worker</p>
<p>apc-lgdcdevl202: worker</p></blockquote>
<p>4、假设你的HDFS已经安装好，并且已经启动（如果不知道怎么安装，需要看我以前的Hadoop文章）</p>
<p>&#160;</p>
<p><strong>配置：</strong></p>
<p>1. 在conf/slaves文件中添加对应的host列表,  启动时会在这里配置的所有机器上启动worker进程。</p>
<blockquote><p><code>[szuser@apc-lgdcdevl201 conf]$ cat slaves<br />
# A Spark Worker will be started on each of the machines listed below.<br />
apc-lgdcdevl201<br />
apc-lgdcdevl202<br />
</code></p></blockquote>
<p>2.在conf/spark-env.sh文件中添加对应的HADOOP配置，主要用于读取hdfs的数据。</p>
<blockquote><p>export JAVA_HOME=/usr/local/jdk1.7.0_76<br />
export SCALA_HOME=<br />
export SPARK_HOME=/home/szuser/ahu/spark/<br />
export SPARK_MASTER_IP=10.12.76.127<br />
export SPARK_WORKER_MEMORY=1g<br />
export HADOOP_CONF_DIR=/home/szuser/ahu/hadoop-cluster/etc/hadoop<br />
export SPARK_LIBRARY_PATH=$SPARK_HOME/lib<br />
export SCALA_LIBRARY_PATH=$SPARK_LIBRARY_PATH</p></blockquote>
<p>配置完成后，把代码分别copy到apc-lgdcdevl201，apc-lgdcdevl202</p>
<p>&#160;</p>
<p><strong>启动Spark集群：</strong></p>
<p>执行sbin/start-all.sh   通过jps命令查看Master和Worker进程是否启动，同时也可以访问master的UI，端口默认是8080，如何被占用了就是8081</p>
<p><a href="http://ahuoo.com/wp-content/uploads/2016/07/Image-6.png"><img class="alignnone size-full wp-image-1917" src="http://ahuoo.com/wp-content/uploads/2016/07/Image-6.png" alt="Image 6" width="1797" height="730" /></a></p>
<p>&#160;</p>
<p>&#160;</p>
<p>如果你能访问这个UI，说明你已经启动成功了。</p>
<p>下面我们来开始使用一下spark了，不要鸡冻， 先进入spark-shell( 一个交互式的scala代码运行窗口)，输入如下命令进入：</p>
<blockquote><p>[szuser@apc-lgdcdevl201 spark]$ MASTER=spark://10.12.76.127:7077 bin/spark-shell</p></blockquote>
<p><strong>我们来写段小代码，任务是：尝试从hdfs读取文件，然后输出第一行。</strong></p>
<p>先用如下命令查看hdfs里面有什么文件</p>
<blockquote><p>[szuser@apc-lgdcdevl202 spark]$ hdfs dfs -ls -R /</p></blockquote>
<p>发现在user/szuser目录里面有个output/part-r-00000文件，然后在shell窗口输入如下代码：</p>
<blockquote><p>scala&#62;  val textFile = sc.textFile(&#8220;output/part-r-00000&#8243;)</p>
<p>scala&#62; textFile.first()</p></blockquote>
<p>此时你就可以读取hdfs的文件内容到textFile变量里面，然后打印出第一行，注意默认是读取的user/szuser目录下面的文件，类似linux文件系统默认是进入自己的home目录。当然spark也是可以从本地读取文件，如果你没有安装hdfs也可以通过如下代码测试：</p>
<blockquote><p>scala&#62;  val textFile = sc.textFile(&#8220;file:////home/szuser/ahu/spark/README.md&#8221;)</p>
<p>scala&#62; textFile.first()</p></blockquote>
<p>&#160;</p>
<p>&#160;</p>
<p><strong>关闭Spark：</strong></p>
<p>执行sbin/stop-all.sh  会关闭Master 和Worker进程。</p>
<p>&#160;</p>
<p><strong>总结： </strong>可以发现spark的安装部署是比较简单的，使用也还可以，Spark主要是以RDD为算子，然后针对这个RDD进行各种操作。</p>
